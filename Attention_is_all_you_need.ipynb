{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libaries"
      ],
      "metadata": {
        "id": "R8xXM7cHXWYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "Q04tLSmJXbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Utilities & config"
      ],
      "metadata": {
        "id": "G_QPcRQ2_2XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "PAD_ID = 0\n",
        "SOS_ID = 1\n",
        "EOS_ID = 2\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    enc_vocab_size: int\n",
        "    dec_vocab_size: int\n",
        "    max_len: int = 64\n",
        "    d_model: int = 128\n",
        "    n_head: int = 8\n",
        "    ffn_hidden: int = 512\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.1\n",
        "    warmup_steps: int = 4000"
      ],
      "metadata": {
        "id": "irgLBdMH_3GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "zprQc-5o_-xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float)\n",
        "            * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe) # [max_len, d_model]\n",
        "\n",
        "    def forward(self, length: int) -> torch.Tensor:\n",
        "        return self.pe[:length].unsqueeze(0) # [1, length, d_model]"
      ],
      "metadata": {
        "id": "kHNYuh9DACD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token + Positional Embedding"
      ],
      "metadata": {
        "id": "1LLYBaOdFK02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_len: int, dropout: float, pad_idx: int = PAD_ID):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = PositionalEncoding(d_model, max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, L]\n",
        "        B, L = x.size()\n",
        "        tok = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        pos = self.pos_embedding(L).to(x.device) # [1, L, d_model]\n",
        "        out = tok + pos\n",
        "        return self.dropout(out)"
      ],
      "metadata": {
        "id": "z-SSxawNFPdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "Y8vNYF5HHhxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # q, k, v [B, H, L, D]\n",
        "        B, H, Lq, D = q.size()\n",
        "        # k_t [B, H, D, L_k]\n",
        "        k_t = k.transpose(2, 3)\n",
        "        score = (q @ k_t) / math.sqrt(D)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.softmax(score)\n",
        "        out = attn @ v\n",
        "        return out, attn"
      ],
      "metadata": {
        "id": "oXR_9svnHjV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention"
      ],
      "metadata": {
        "id": "xyBrPwosPcAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_head: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.d_head = d_model // n_head\n",
        "        self.scale_dot_product_attention = ScaleDotProductAttention()\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x [B, L, D] -> [B, H, L, D]\n",
        "        B, L, D = x.size()\n",
        "        return x.view(B, L, self.n_head, self.d_head).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x [B, H, L, D] -> [B, L, D]\n",
        "        B, H, L, D = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # q, k, v [B, L, D]\n",
        "        q = self.split_heads(self.W_q(q))\n",
        "        k = self.split_heads(self.W_k(k))\n",
        "        v = self.split_heads(self.W_v(v))\n",
        "        out, attn = self.attn(q, k, v, mask=mask)\n",
        "        out = self.combine_heads(out)\n",
        "        out = self.dropout(self.W_o(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "H92r9ky7PedM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LayerNorm & FFN"
      ],
      "metadata": {
        "id": "X_Bot5g5TMDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True)\n",
        "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.gamma * x_hat + self.beta"
      ],
      "metadata": {
        "id": "XG_0fZSrTQBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, hidden: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "_EWLgouhUEuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder / Decoder Layers"
      ],
      "metadata": {
        "id": "ZTvK8fdoUaHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head, drop_prob)\n",
        "        self.dropout1 = nn.Dropout(drop_prob)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
        "        self.dropout2 = nn.Dropout(drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # Self-attention\n",
        "        _x = x\n",
        "        x = self.self_attn(x, x, x, mask=src_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        # FFN\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fVB4OdyXUfS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head, drop_prob)\n",
        "        self.dropout1 = nn.Dropout(drop_prob)\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, n_head, drop_prob)\n",
        "        self.dropout2 = nn.Dropout(drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
        "        self.dropout3 = nn.Dropout(drop_prob)\n",
        "        self.norm3 = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        # Masked self-attention (look-ahead + pad)\n",
        "        _x = dec\n",
        "        x = self.self_attn(dec, dec, dec, mask=trg_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        # Encoder-Decoder attention\n",
        "        _x = x\n",
        "        x = self.enc_dec_attn(x, enc, enc, mask=src_mask)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "\n",
        "        # FFN\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iqlXzZw4U1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder / Decoder Stacks"
      ],
      "metadata": {
        "id": "SfmQmJMVU4e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, pad_idx=PAD_ID):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(vocab_size, d_model, max_len, drop_prob, pad_idx)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, ffn_hidden, n_head, drop_prob) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.emb(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x  # [B, L, d_model]"
      ],
      "metadata": {
        "id": "FvpFcld3U6aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, pad_idx=PAD_ID):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(vocab_size, d_model, max_len, drop_prob, pad_idx)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, ffn_hidden, n_head, drop_prob) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, trg, enc, trg_mask, src_mask):\n",
        "        x = self.emb(trg)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc, trg_mask, src_mask)\n",
        "        return self.proj(x)  # logits: [B, L, vocab]"
      ],
      "metadata": {
        "id": "YHiFtcXgU_JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Transformer Model"
      ],
      "metadata": {
        "id": "6wolyJ9AVULV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig, pad_idx=PAD_ID):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(cfg.enc_vocab_size, cfg.max_len, cfg.d_model,\n",
        "                               cfg.ffn_hidden, cfg.n_head, cfg.n_layers, cfg.dropout, pad_idx)\n",
        "        self.decoder = Decoder(cfg.dec_vocab_size, cfg.max_len, cfg.d_model,\n",
        "                               cfg.ffn_hidden, cfg.n_head, cfg.n_layers, cfg.dropout, pad_idx)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def make_src_mask(self, src: torch.Tensor) -> torch.Tensor:\n",
        "        # src: [B, Ls] -> mask over keys: [B, 1, 1, Ls] (1=keep, 0=mask)\n",
        "        mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2).int()\n",
        "        return mask  # broadcast to [B, H, Lq, Lk]\n",
        "\n",
        "    def make_trg_mask(self, trg_in: torch.Tensor) -> torch.Tensor:\n",
        "        # trg_in: [B, Lt]\n",
        "        B, Lt = trg_in.size()\n",
        "        pad_mask = (trg_in != self.pad_idx).unsqueeze(1).unsqueeze(2).int()  # [B,1,1,Lt]\n",
        "        # look-ahead mask: lower-triangular (allow attending to <= current position)\n",
        "        sub_mask = torch.tril(torch.ones((Lt, Lt), device=trg_in.device)).unsqueeze(0).unsqueeze(0).int()  # [1,1,Lt,Lt]\n",
        "        return pad_mask & sub_mask  # [B,1,Lt,Lt], 1=keep, 0=mask\n",
        "\n",
        "    def forward(self, src: torch.Tensor, trg_in: torch.Tensor) -> torch.Tensor:\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg_in)\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        logits = self.decoder(trg_in, memory, trg_mask, src_mask)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src: torch.Tensor, max_len: int, sos_id=SOS_ID, eos_id=EOS_ID) -> torch.Tensor:\n",
        "        # src: [B, Ls]\n",
        "        self.eval()\n",
        "        B = src.size(0)\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        ys = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
        "        for _ in range(max_len - 1):\n",
        "            trg_mask = self.make_trg_mask(ys)\n",
        "            out = self.decoder(ys, memory, trg_mask, src_mask)  # [B, L, V]\n",
        "            next_token = out[:, -1, :].argmax(dim=-1, keepdim=True)  # [B,1]\n",
        "            ys = torch.cat([ys, next_token], dim=1)\n",
        "            if (next_token == eos_id).all():\n",
        "                break\n",
        "        return ys"
      ],
      "metadata": {
        "id": "yJ38F_NmVX7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}